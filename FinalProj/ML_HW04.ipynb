{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copyright (C) 2022 Sobhan Moradian Daghigh and s.o who's unknown :)\n",
    "### Date: 2/20/2022 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "id": "mBMoPLmGbrIn"
   },
   "outputs": [],
   "source": [
    "from amalearn.reward import RewardBase\n",
    "from amalearn.agent import AgentBase\n",
    "from amalearn.environment import EnvironmentBase\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {
    "id": "pH6sNHxPbrIs"
   },
   "outputs": [],
   "source": [
    "class Environment(EnvironmentBase):\n",
    "    \n",
    "    obstacles = [(1, 7), (1, 8), (2, 7), (2, 8), (3, 7), (3, 8), (4, 7), (4, 8),\n",
    "                (12, 6), (12, 7), (13, 6), (13, 7), (14, 6), (14, 7), (15, 6), (15, 7),\n",
    "                (8, 13), (8, 14), (8, 15), (9, 13), (9, 14), (9, 15)]\n",
    "    \n",
    "    def __init__(self, actionPrice, goalReward, punish, obstacles=obstacles,\n",
    "                 i_limit=15, j_limit=15, p=0.8, goal=(1, 1), start=(15, 15), container=None):\n",
    "        \"\"\" initialize your variables \"\"\"\n",
    "        \n",
    "        state_space = gym.spaces.MultiDiscrete([i_limit, j_limit])\n",
    "        action_space = gym.spaces.Discrete(9)\n",
    "        super(Environment, self).__init__(action_space, state_space, container)\n",
    "        self.state_space = state_space\n",
    "        self.obstacles = obstacles\n",
    "        self.actionPrice = actionPrice\n",
    "        self.goalReward = goalReward\n",
    "        self.punish = punish\n",
    "        self.i_limit = i_limit\n",
    "        self.j_limit = j_limit\n",
    "        self.p = p\n",
    "        self.goal = goal\n",
    "        self.start = start\n",
    "        self.state = start\n",
    "        self.state_p = None\n",
    "        \n",
    "        \n",
    "    # -------------------------------------------------------------------------------------------------------------    \n",
    "    def isStatePossible(self, state):\n",
    "        \"\"\"if given state is possible (not out of the grid and not obstacle) return ture\"\"\"\n",
    "        \n",
    "        i_in = range(1, self.i_limit + 1)\n",
    "        j_in = range(1, self.j_limit + 1)\n",
    "        \n",
    "        return False if state in self.obstacles or state[0] not in i_in or state[1] not in j_in else True\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def isAccessible(self, state, state_p):\n",
    "        \"\"\"if given state is Accesible (we can reach state_p by doing an action from state) return true\"\"\"\n",
    "\n",
    "        if self.isStatePossible(state) and self.isStatePossible(state_p):\n",
    "            if (np.abs(np.subtract(state, state_p)) <= (1, 1)).all():\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def getTransitionStatesAndProbs(self, state, action, state_p):\n",
    "        \"\"\"return probability of transition or T(sp,a,s)\"\"\"\n",
    "\n",
    "        _, actions = self.available_actions(state)\n",
    "        if action in actions:\n",
    "            available_states = self.available_states(actions, state)\n",
    "            \n",
    "            if self.next_state(action, state) == state_p:\n",
    "                return self.p\n",
    "            \n",
    "            elif state_p in available_states:\n",
    "                return (1 - self.p) / (len(available_states) - 1)\n",
    "            \n",
    "            else: return 0\n",
    "        else: return 0\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def getReward(self, state, action, state_p):\n",
    "        \"\"\"return reward of transition\"\"\"\n",
    "        \n",
    "        # The Goal Achieved\n",
    "        if state_p == self.goal:\n",
    "            return self.goalReward\n",
    "        \n",
    "        elif self.isAccessible(state, state_p):\n",
    "            return self.actionPrice\n",
    "        \n",
    "        # Hit the obstacles\n",
    "        else:\n",
    "            return self.punish\n",
    "        \n",
    "            \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def calculate_reward(self, action):\n",
    "#         probabilities = []\n",
    "#         for i in range(-1, 2):\n",
    "#             for j in range(-1, 2):\n",
    "#                 state_p = self.next_state((i, j), self.state)\n",
    "#                 probabilities.append(self.getTransitionStatesAndProbs(self.state, action, state_p))\n",
    "        \n",
    "#         _, available_actions = self.available_actions(self.state)\n",
    "#         move = available_actions[np.random.choice(len(available_actions), p=list(filter(lambda num: num != 0, probabilities)))]\n",
    "#         self.state_p = tuple(np.add(move, self.state))\n",
    "#         return self.state_p\n",
    "        pass\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def terminated(self, state):\n",
    "        return state == self.goal\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def available_actions(self, state):\n",
    "        actions = []\n",
    "        numbers = []\n",
    "        num = 0\n",
    "        for i in range(-1, 2):\n",
    "            for j in range(-1, 2):\n",
    "                state_p = tuple(np.add(state, (j, i)))\n",
    "                if self.isAccessible(state, state_p):\n",
    "                    actions.append((j, i))\n",
    "                    numbers.append(num)\n",
    "                num += 1\n",
    "                    \n",
    "        return numbers, actions\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def action_num2dim(self, num):\n",
    "        if num < 3:\n",
    "            return (np.mod(num, 3) -1, -1)\n",
    "        elif num < 6:\n",
    "            return (np.mod(num, 3) -1, 0)\n",
    "        else:\n",
    "            return (np.mod(num, 3) -1, 1)\n",
    "        \n",
    "        \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def get_states(self):\n",
    "        states = []\n",
    "        for i in range(self.i_limit, 0, -1):\n",
    "            for j in range(self.j_limit, 0, -1):\n",
    "                if self.isStatePossible((i, j)):\n",
    "                    states.append((i, j))\n",
    "        return states\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def available_states(self, actions, state):\n",
    "        states_p = []\n",
    "        for action in actions:\n",
    "            states_p.append(self.next_state(action, state))\n",
    "        return states_p\n",
    "                \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def next_state(self, action, state):\n",
    "        return tuple(np.add(state, action))\n",
    "\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        \n",
    "        \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def observe(self):\n",
    "        return self.state\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def render(self):\n",
    "        return \n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def close(self):\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {
    "id": "898Jlhsycyes"
   },
   "outputs": [],
   "source": [
    "class Agent(AgentBase):\n",
    "    def __init__(self, environment, theta=0.1, discount=0.9):\n",
    "        #initialize a random policy and V(s) = 0 for each state\n",
    "        self.environment = environment\n",
    "\n",
    "        self.width  = self.environment.i_limit\n",
    "        self.height = self.environment.j_limit\n",
    "        \n",
    "        #init V\n",
    "        self.V = [[0] * (self.width + 1) for _ in range(self.height + 1)]\n",
    "        self.V[1][1] = self.environment.goalReward\n",
    "        \n",
    "        #init policy\n",
    "        self.policy = np.random.randint(0, 9, (self.width + 1, self.height + 1))\n",
    "        \n",
    "        super(Agent, self).__init__(id, environment)\n",
    "        self.discount = discount\n",
    "        self.theta = theta\n",
    "        \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def policy_evaluation(self):        \n",
    "        while True:\n",
    "            delta = 0.01 #TODO\n",
    "            pre_delta = delta\n",
    "            for state in self.environment.get_states():\n",
    "                v = self.V[state[0]][state[1]]                \n",
    "                action = self.policy[state[0]][state[1]]                \n",
    "                numbers, actions = self.environment.available_actions(state)\n",
    "                value = 0\n",
    "                for act in actions:\n",
    "                    state_p = self.environment.next_state(act, state)\n",
    "                    reward = self.environment.getReward(state, action_num2dim(action), state_p)\n",
    "                    probability = self.environment.getTransitionStatesAndProbs(state, action_num2dim(action), state_p)\n",
    "                    value += probability * (reward + self.discount * self.V[state_p[0]][state_p[1]])\n",
    "\n",
    "                self.V[state[0]][state[1]] = value\n",
    "                delta = max([delta, np.abs(v - self.V[state[0]][state[1]])])\n",
    "\n",
    "            if delta < self.theta or delta == pre_delta:\n",
    "                break\n",
    "                \n",
    "            pre_delta = delta\n",
    "                \n",
    "        return self.V\n",
    "                    \n",
    "        \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def policy_improvement(self):\n",
    "        unchanged = True\n",
    "        for state in self.environment.get_states():\n",
    "            pre_action = self.policy[state[0]][state[1]]\n",
    "            acts = []\n",
    "            numbers, actions = self.environment.available_actions(state)\n",
    "            for _, act1 in zip(numbers, actions):\n",
    "                value = 0\n",
    "                for _, act2 in zip(numbers, actions):\n",
    "                    state_p = self.environment.next_state(act2, state)\n",
    "                    reward = self.environment.getReward(state, act1, state_p)                    \n",
    "                    probability = self.environment.getTransitionStatesAndProbs(state, act1, state_p)\n",
    "                    value += probability * (reward + self.discount * self.V[state_p[0]][state_p[1]])\n",
    "                acts.append(value)\n",
    "            best_act = np.argmax(acts)\n",
    "            self.policy[state[0]][state[1]] = numbers[best_act]\n",
    "            \n",
    "            if numbers[best_act] != pre_action:\n",
    "                unchanged = False\n",
    "        return unchanged\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def policy_iteration(self):\n",
    "        unchanged = False\n",
    "        while not unchanged:\n",
    "            self.V = self.policy_evaluation()\n",
    "            unchanged = self.policy_improvement()\n",
    "            \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def take_action(self) -> (object, float, bool, object):\n",
    "        self.policy_iteration()\n",
    "                    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def visualize_policy(self):\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        policy = self.policy[1:, 1:].T\n",
    "\n",
    "        ax.matshow(policy)\n",
    "        for i in range(self.width):\n",
    "            for j in range(self.height):\n",
    "                c = policy[i, j]\n",
    "                ax.text(i, j, str(c), va='center', ha='center')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def visualize_values(self):\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        value = np.delete(self.V, 0, axis=1)[1::].T\n",
    "\n",
    "        ax.matshow(value, cmap='summer')\n",
    "        for i in range(self.width):\n",
    "            for j in range(self.height):\n",
    "                c = round(value[i][j])\n",
    "                ax.text(i, j, str(c), va='center', ha='center')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    def get_path(self):\n",
    "        path = []\n",
    "        states = []\n",
    "        start = self.environment.start\n",
    "        curr = start\n",
    "        while True:\n",
    "            num = self.policy[curr[0]][curr[1]]\n",
    "            path.append(num)\n",
    "            states.append(curr)\n",
    "            direction = self.environment.action_num2dim(num)\n",
    "            curr = tuple(np.add(curr, direction))\n",
    "            if self.environment.terminated(curr):\n",
    "                break\n",
    "        return path, states[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(environment, agent):\n",
    "    plt.gcf().set_size_inches(10, 10)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    ax.set_xticks(range(1, environment.i_limit + 1))\n",
    "    ax.set_yticks(range(environment.j_limit, 0, -1))\n",
    "    ax.axes.xaxis.set_ticklabels([])\n",
    "    ax.axes.yaxis.set_ticklabels([])\n",
    "    \n",
    "    path, states = agent.get_path()\n",
    "    no = 1\n",
    "    for i in range(environment.i_limit, 0, -1):\n",
    "        temp = (environment.i_limit + 1) - i\n",
    "        for j in range(environment.j_limit, 0, -1):\n",
    "            if (i, j) == environment.start:\n",
    "                plt.gca().text(j - 0.5, temp - 0.5, str(no), va='center', ha='center')\n",
    "                plt.gca().add_patch(Rectangle((j, temp), -1, -1, fill=True, color='purple', alpha=0.5))\n",
    "                no += 1\n",
    "            \n",
    "            if (i, j) in environment.obstacles:\n",
    "                plt.gca().add_patch(Rectangle((j, temp), -1, -1, fill=True, color='black', alpha=0.5))\n",
    "                \n",
    "            elif (i, j) in states:\n",
    "                num = path[states.index((i, j))]\n",
    "                direction = environment.action_num2dim(num)\n",
    "                \n",
    "                plt.gca().text(j - 0.5, temp - 0.5, str(no), va='center', ha='center')\n",
    "                plt.gca().add_patch(Rectangle((j, temp), -1, -1, fill=True, color='blue', alpha=0.5))\n",
    "                no += 1\n",
    "                \n",
    "            elif (i, j) == environment.goal:\n",
    "                plt.gca().text(j - 0.5, temp - 0.5, str(no), va='center', ha='center')\n",
    "                plt.gca().add_patch(Rectangle((j, temp), -1, -1, fill=True, color='green', alpha=0.5))\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "environment = Environment(actionPrice = -0.01, goalReward= 1000, punish = -1)\n",
    "agent = Agent(environment, theta=0.01)\n",
    "agent.take_action()\n",
    "plotter(environment, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "env.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "83fad98a7911d3a2a55c2e5234aea09e74d0252d0d10d90172c6e09f21426bdf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
